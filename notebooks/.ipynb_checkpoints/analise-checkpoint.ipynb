{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db93cb3-141f-4d52-bab4-29c1c4cd97af",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aee5179-fec1-4b3d-af94-9537d6fb59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329a629c-1eef-42c2-870e-985ed5270f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junior/Documentos/palestra_scti/venv/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base              import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics           import roc_curve, precision_recall_curve\n",
    "from sklearn.inspection        import permutation_importance\n",
    "from sklearn.calibration       import calibration_curve\n",
    "from sklearn.model_selection   import learning_curve, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4311d110-c752-4afd-bef5-fe851a56f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats  import ks_2samp\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    r2_score, \n",
    "    mean_absolute_error, \n",
    "    mean_absolute_percentage_error, \n",
    "    root_mean_squared_error, \n",
    "    explained_variance_score, \n",
    "    f1_score, \n",
    "    make_scorer,\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    brier_score_loss,\n",
    "    median_absolute_error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae68d142-4d0f-4e24-a890-7768905d0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a4e9f6-c4e9-4a7b-bda0-164addc4fb87",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c06e1ef-57d5-499d-a7b5-6e039df6f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_score(y_true: pd.Series, y_prob: pd.Series) -> float:\n",
    "    \n",
    "    pos_prob = y_prob[y_true == 1]\n",
    "    neg_prob = y_prob[y_true == 0]\n",
    "    \n",
    "    if len(pos_prob) == 0 or len(neg_prob) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    result = ks_2samp(pos_prob, neg_prob)\n",
    "    \n",
    "    return result.statistic\n",
    "\n",
    "ks_scorer = make_scorer(ks_score, greater_is_better=True, response_method=\"predict_proba\")\n",
    "\n",
    "def get_eval_scoring(scoring, return_func=True):\n",
    "    \n",
    "    scorers = {\n",
    "        'r2': 'r2',\n",
    "        'ks': ks_scorer,\n",
    "        'brier': 'neg_brier_score',\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'explained_variance': 'explained_variance',\n",
    "        'mean_absolute_error': 'neg_mean_absolute_error',\n",
    "        'median_absolute_error': 'neg_median_absolute_error',\n",
    "        'root_mean_squared_error': 'neg_root_mean_squared_error',\n",
    "        'mean_absolute_percentage_error': 'neg_mean_absolute_percentage_error'\n",
    "    }\n",
    "\n",
    "    functions = {\n",
    "        'r2': r2_score,\n",
    "        'ks': ks_score,\n",
    "        'brier': lambda y_true, y_pred: -1 * brier_score_loss(y_true, y_pred),\n",
    "        'roc_auc': roc_auc_score,\n",
    "        'explained_variance': explained_variance_score,\n",
    "        'mean_absolute_error': lambda y_true, y_pred: -1 * mean_absolute_error(y_true, y_pred),\n",
    "        'median_absolute_error': lambda y_true, y_pred: -1 * median_absolute_error(y_true, y_pred),\n",
    "        'root_mean_squared_error': lambda y_true, y_pred: -1 * root_mean_squared_error(y_true, y_pred),\n",
    "        'mean_absolute_percentage_error': lambda y_true, y_pred: -1 * mean_absolute_percentage_error(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    if scoring not in scorers:\n",
    "        raise ValueError(f\"Metric '{scoring}' is not supported.\")\n",
    "\n",
    "    if scoring not in functions:\n",
    "        raise ValueError(f\"Metric '{scoring}' is not supported.\")\n",
    "\n",
    "    return functions[scoring] if return_func else scorers[scoring]\n",
    "\n",
    "def analyze_model(model_name: str, model: BaseEstimator, results: dict, X_train: pd.DataFrame, y_train: pd.DataFrame, y_test: pd.DataFrame, target: str) -> None:\n",
    "    \n",
    "    print(f\"{model_name} Results\")\n",
    "\n",
    "    display(summarize_metric_results(results))\n",
    "\n",
    "    pred_col = f\"{model_name}_pred\"\n",
    "    plot_residuals(y_test, pred_col, f\"{model_name} (Test Dataset)\", target)\n",
    "    plot_pred_vs_true(y_test, pred_col, f\"{model_name} (Test Dataset)\", target)\n",
    "    plot_error_by_quantile(y_test, pred_col, f\"{model_name} (Test Dataset)\", target)\n",
    "    plot_feature_importance(model)\n",
    "    plot_permutation_importance(model, X_train, y_train[target])\n",
    "    plot_shap_summary(model, X_train)\n",
    "\n",
    "def get_regressor_metrics(y: pd.DataFrame, pred_col: str, target: str) -> dict[str, float]:\n",
    "        \n",
    "    y_true = y[target]\n",
    "    y_pred = y[pred_col]\n",
    "    \n",
    "    return {\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'MadAE': median_absolute_error(y_true, y_pred),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred),\n",
    "        'RMSE': root_mean_squared_error(y_true, y_pred),\n",
    "        'Explained Variance': explained_variance_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def get_best_threshold_for_f1(y_true: pd.Series, y_probs: pd.Series) -> float:\n",
    "    \n",
    "    thresholds = np.linspace(0, 1, 200)\n",
    "    best_thresh = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        \n",
    "        y_pred = (y_probs >= thresh).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            \n",
    "            best_f1 = f1\n",
    "            best_thresh = thresh\n",
    "            \n",
    "    return best_thresh\n",
    "\n",
    "def get_classifier_metrics(y: pd.DataFrame, model_name: str, target: str) -> dict[str, float]:\n",
    "        \n",
    "    best_threshold = get_best_threshold_for_f1(y[target], y[f'{model_name}_prob'])\n",
    "\n",
    "    y[f'{model_name}_pred'] = 0\n",
    "    y.loc[y[f'{model_name}_prob'] >= best_threshold, f'{model_name}_pred'] = 1\n",
    "    \n",
    "    return {\n",
    "        'Treshold': best_threshold,\n",
    "        'Balanced Accuracy': balanced_accuracy_score(y[target], y[f'{model_name}_pred']),\n",
    "        'Precision': precision_score(y[target], y[f'{model_name}_pred']),\n",
    "        'Recall': recall_score(y[target], y[f'{model_name}_pred']),\n",
    "        'F1': f1_score(y[target], y[f'{model_name}_pred']),\n",
    "        'AUC': roc_auc_score(y[target], y[f'{model_name}_prob']),\n",
    "        'KS': ks_score(y[target], y[f'{model_name}_prob']),\n",
    "        'Brier': brier_score_loss(y[target], y[f'{model_name}_prob'])\n",
    "    }\n",
    "\n",
    "def get_default_model(random_state) -> LGBMClassifier:\n",
    "    \n",
    "    return LGBMClassifier(random_state=random_state, verbose=-1, objective='binary')\n",
    "\n",
    "def get_params_objective(trial, random_state=42) -> dict:\n",
    "\n",
    "    return {\n",
    "        'objective': trial.suggest_categorical('objective', ['binary']),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt']),\n",
    "        'metric': trial.suggest_categorical('metric', ['auc', 'binary_logloss']),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'random_state': trial.suggest_categorical('random_state', [random_state]),\n",
    "        'verbose': trial.suggest_categorical('verbose', [-1])\n",
    "}\n",
    "\n",
    "\n",
    "def prob_to_score(probs: pd.Series, min_score: int = 0, max_score: int = 1000, inverse: bool = False) -> np.array:\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert probabilities to scores in intervals [min_score, max_score].\n",
    "\n",
    "    Args:\n",
    "        probs (array-like): List of probabilities between 0 and 1.\n",
    "        min_score (int): Scor min (default=1).\n",
    "        max_score (int): Scor max (default=1000).\n",
    "        inverse (bool): If True, probability is high => score is lower (ex: risk).\n",
    "\n",
    "    Returns:\n",
    "        np.array: Scores between min_score and max_score.\n",
    "    \"\"\"\n",
    "\n",
    "    probs = np.asarray(probs)\n",
    "\n",
    "    if inverse:\n",
    "        probs = 1 - probs\n",
    "    \n",
    "    scores = min_score + (max_score - min_score) * probs\n",
    "    \n",
    "    return np.round(scores).astype(int)\n",
    "\n",
    "def calc_rating_limits(y: pd.Series, n_ratings: int = 5, min_score: int = 0, max_score: int = 1000) -> list[float]:\n",
    "    \n",
    "    scores = np.asarray(y)\n",
    "    \n",
    "    quantiles = np.linspace(0, 1, n_ratings + 1)[1:-1]\n",
    "    inner_thresholds = np.quantile(scores, quantiles)\n",
    "    \n",
    "    thresholds = [min_score] + inner_thresholds.tolist() + [max_score]\n",
    "\n",
    "    return thresholds\n",
    "\n",
    "def apply_ratings(y: pd.Series, thresholds: list[float], labels: list[str] = None) -> pd.Series:\n",
    "  \n",
    "    return pd.cut(y, bins=thresholds, labels=labels, include_lowest=True)\n",
    "\n",
    "def plot_rating_distribution(y: pd.Series, model_name: str, target: str) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Plot the distribution of ratings.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with the rating and score.\n",
    "    \"\"\"\n",
    "    \n",
    "    y.groupby([f'{model_name}_rating'], observed=True)[target].count().plot(kind='bar')\n",
    "    \n",
    "    plt.title(f'Rating Distribution - {model_name}')\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def plot_rating(y: pd.Series, model_name: str, target: str) -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot the average score for each rating.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with the rating and score.\n",
    "    \"\"\"\n",
    "    \n",
    "    y.groupby([f'{model_name}_rating'], observed=True)[target].mean().plot(kind='bar')\n",
    "    \n",
    "    plt.title(f'Rating vs Target - {model_name}')\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Average Target')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def summarize_metric_results(results: dict[str, dict[str, float]]) -> pd.DataFrame:\n",
    "        \n",
    "    rows = []\n",
    "    \n",
    "    for dataset, metrics_dict in results.items():\n",
    "        row = {\"Dataset\": dataset}\n",
    "        row.update(metrics_dict)\n",
    "        rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def detect_model_type(model) -> str:\n",
    "    \n",
    "    name = type(model).__name__.lower()\n",
    "    \n",
    "    if 'classifier' in name or hasattr(model, 'predict_proba'):\n",
    "        return 'classifier'\n",
    "    \n",
    "    elif 'regressor' in name or (hasattr(model, 'predict') and not hasattr(model, 'predict_proba')):\n",
    "        return 'regressor'\n",
    "    \n",
    "    return 'unknown'\n",
    "\n",
    "def analyze_model(model_name: str, model: BaseEstimator, results: dict, features: list, X_train: pd.DataFrame, y_train: pd.DataFrame, y_test: pd.DataFrame, target: str, scoring: str) -> None:\n",
    "    \n",
    "    print(f\"{model_name} Results\")\n",
    "\n",
    "    display(summarize_metric_results(results))\n",
    "\n",
    "    if detect_model_type(model) == 'regressor':\n",
    "        pred_col = f\"{model_name}_pred\"\n",
    "        \n",
    "        plot_residuals(y_test, pred_col, target)\n",
    "        plot_pred_vs_true(y_test, pred_col, target)\n",
    "        plot_error_by_quantile(y_test, pred_col, target)\n",
    "\n",
    "    elif detect_model_type(model) == 'classifier':\n",
    "        pred_col = f\"{model_name}_pred\"\n",
    "        prob_col = f\"{model_name}_prob\"\n",
    "        \n",
    "        plot_rating(y_test, model_name, target)\n",
    "        plot_rating_distribution(y_test, model_name, target)\n",
    "        plot_roc_curve(y_test, prob_col, target)\n",
    "        plot_precision_recall_curve(y_test, prob_col, target)\n",
    "        plot_calibration_curve(y_test, model_name, target, strategy='uniform')\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Model needs to be a classifier or regressor.\")\n",
    "\n",
    "    plot_learning_curve(model, X_train, y_train, target, scoring=scoring)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        plot_feature_importance(model)\n",
    "    plot_permutation_importance(model, features, X_train, y_train[target], scoring=scoring)\n",
    "    plot_shap_summary(model, features, X_train)\n",
    "\n",
    "def plot_roc_curve(y: pd.DataFrame, prob_col: str, target: str) -> None:\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y[target], y[prob_col])\n",
    "\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.title('ROC AUC Curve')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curve(y: pd.DataFrame, prob_col: str, target: str):\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y[target], y[prob_col])\n",
    "\n",
    "    plt.plot(precision, recall)\n",
    "    plt.title('Precision Recall Curve')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curve(model: BaseEstimator, X: pd.DataFrame, y: pd.DataFrame, target: str, scoring: str, cv: int = 5):\n",
    "\n",
    "    if isinstance(model, ClassifierMixin):\n",
    "        splitter = StratifiedKFold(cv, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        splitter = KFold(cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "        model, X, y[target],\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=splitter,\n",
    "        scoring=scoring\n",
    "    )\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes_abs, train_scores_mean, 'o-', color='r', label='Train')\n",
    "    plt.fill_between(\n",
    "        train_sizes_abs, train_scores_mean - train_scores_std, \n",
    "        train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "\n",
    "    plt.plot(train_sizes_abs, val_scores_mean, 'o-', color='b', label='Validation')\n",
    "    plt.fill_between(\n",
    "        train_sizes_abs, val_scores_mean - val_scores_std,\n",
    "        val_scores_mean + val_scores_std, alpha=0.1, color='b')\n",
    "\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.xlabel(\"Train Size\")\n",
    "    plt.ylabel(f\"{scoring}\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_calibration_curve(y: pd.DataFrame, model_name: str, target: str, n_bins=11, strategy='uniform') -> None:\n",
    "\n",
    "    prob_true, prob_pred = calibration_curve(y[target], y[f\"{model_name}_prob\"], n_bins=n_bins, strategy=strategy)\n",
    "\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated', color='gray')\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Observed Frequency')\n",
    "    plt.title('Calibration Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(model: BaseEstimator) -> None:\n",
    "\n",
    "    feature_names = model.feature_name_ if hasattr(model, 'feature_name_') else model.feature_names_ # for catboost\n",
    "    \n",
    "    df_imp = pd.DataFrame(model.feature_importances_, feature_names).reset_index()\n",
    "    df_imp.columns = [\"Variable\", \"Importance\"]\n",
    "    df_imp = df_imp.sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "    sns.barplot(x=\"Importance\", y=\"Variable\", color=\"#006e9cff\", data=df_imp[:20])\n",
    "\n",
    "    plt.title(f\"Importance of Variables\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_permutation_importance(model: BaseEstimator, features: list, X: pd.DataFrame, y: pd.DataFrame, scoring: str) -> None:\n",
    "\n",
    "    permu_results = permutation_importance(model, X[features], y, scoring=scoring, n_repeats=5, random_state=42)\n",
    "\n",
    "    sorted_importances_idx = permu_results.importances_mean.argsort()\n",
    "    \n",
    "    df_results = pd.DataFrame(permu_results.importances[sorted_importances_idx].T, columns=X.columns[sorted_importances_idx])\n",
    "    \n",
    "    ax = df_results.plot.box(vert=False, whis=10, patch_artist=True, boxprops={'facecolor':'skyblue', 'color':'blue'})\n",
    "    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "    ax.set_xlabel(f\"Decrease in {scoring}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_shap_summary(model: BaseEstimator, features: list, X: pd.DataFrame) -> None:\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X[features])\n",
    "    \n",
    "    shap.summary_plot(shap_values, X[features])\n",
    "\n",
    "class AutoMLLGBMClassifierCV:\n",
    "\n",
    "    def __init__(\n",
    "        self, X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame, best_features: list[str], \n",
    "        target: str, scoring: str, n_trials: int = 50, cv: int = 5, random_state: int = 42):\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.best_features = best_features\n",
    "        self.target = target\n",
    "        self.n_trials = n_trials\n",
    "        self.cv = cv\n",
    "        self.scorer = get_eval_scoring(scoring, return_func=False)\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _cross_validate(self, model: LGBMClassifier, features: list[str]) -> None:\n",
    "        \n",
    "        cv_results = cross_validate(\n",
    "            estimator=model, \n",
    "            X=self.X_train[features], \n",
    "            y=self.y_train[self.target], \n",
    "            cv=self.cv,\n",
    "            scoring={\n",
    "                'balanced_accuracy': 'balanced_accuracy', \n",
    "                'precision': 'precision', \n",
    "                'recall': 'recall', \n",
    "                'f1': 'f1', \n",
    "                'roc_auc': 'roc_auc', \n",
    "                'ks': ks_scorer, \n",
    "                'brier': 'neg_brier_score'\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'Treshold': 0.5,\n",
    "            'Balanced Accuracy': cv_results['test_balanced_accuracy'].mean(),\n",
    "            'Precision': cv_results['test_precision'].mean(),\n",
    "            'Recall': cv_results['test_recall'].mean(),\n",
    "            'F1': cv_results['test_f1'].mean(),\n",
    "            'AUC': cv_results['test_roc_auc'].mean(),\n",
    "            'KS': cv_results['test_ks'].mean(),\n",
    "            'Brier': np.abs(cv_results['test_brier'].mean())\n",
    "        }\n",
    "\n",
    "    def _train_model(self, model_name: str, features: list[str], model: LGBMClassifier) -> tuple[LGBMClassifier, dict[str, dict[str, float]]]:\n",
    "\n",
    "        model.fit(self.X_train[features], self.y_train[self.target])\n",
    "        \n",
    "        self.y_train[f'{model_name}_prob'] = model.predict_proba(self.X_train[features])[:, 1]\n",
    "        self.y_train[f'{model_name}_score'] = prob_to_score(self.y_train[f'{model_name}_prob'], inverse=True)\n",
    "        self.train_rating_limits = calc_rating_limits(self.y_train[f'{model_name}_score'])\n",
    "        self.y_train[f'{model_name}_rating'] = apply_ratings(self.y_train[f'{model_name}_score'], self.train_rating_limits)\n",
    "        \n",
    "        self.y_test[f'{model_name}_prob'] = model.predict_proba(self.X_test[features])[:, 1]\n",
    "        self.y_test[f'{model_name}_score'] = prob_to_score(self.y_test[f'{model_name}_prob'], inverse=True)\n",
    "        self.y_test[f'{model_name}_rating'] = apply_ratings(self.y_test[f'{model_name}_score'], self.train_rating_limits)\n",
    "        \n",
    "        results = {\n",
    "            'Train CV': self._cross_validate(model, features),\n",
    "            'Test': get_classifier_metrics(self.y_test, model_name, self.target)\n",
    "        }\n",
    "        \n",
    "        return model, results\n",
    "\n",
    "    def _train_base_model(self) -> dict[str, dict[str, float]]:\n",
    "        \n",
    "        model = get_default_model(random_state=self.random_state)\n",
    "        \n",
    "        return self._train_model('base_model', self.X_train.columns.tolist(), model)\n",
    "\n",
    "    def _train_best_feature_model(self) -> dict[str, dict[str, float]]:\n",
    "        \n",
    "        model = get_default_model(random_state=self.random_state)\n",
    "        \n",
    "        return self._train_model('best_feature_model', self.best_features, model)\n",
    "\n",
    "    def _get_best_params(self) -> dict:\n",
    "\n",
    "        def objective(trial):\n",
    "            \n",
    "            params = get_params_objective(trial, random_state=self.random_state)\n",
    "    \n",
    "            cv_results = cross_validate(\n",
    "                estimator=LGBMClassifier(**params), cv=self.cv, scoring=self.scorer,\n",
    "                X=self.X_train[self.best_features], y=self.y_train[self.target])\n",
    "    \n",
    "            return cv_results['test_score'].mean()\n",
    "    \n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=self.n_trials)\n",
    "    \n",
    "        return study.best_params\n",
    "\n",
    "    def _train_best_params_model(self) -> dict[str, dict[str, float]]:\n",
    "        \n",
    "        best_params = self._get_best_params()\n",
    "        self.best_params_model = LGBMClassifier(**best_params)\n",
    "        \n",
    "        return self._train_model('best_params_model', self.best_features, self.best_params_model)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \n",
    "        self.base_model, self.base_model_results = self._train_base_model()\n",
    "        self.best_feature_model, self.best_feature_model_results = self._train_best_feature_model()\n",
    "        self.best_params_model, self.best_params_model_results = self._train_best_params_model()\n",
    "\n",
    "    def get_metrics(self) -> pd.DataFrame:\n",
    "\n",
    "        model_results = {\n",
    "            \"Base Model\": self.base_model_results,\n",
    "            \"Best Feature Model\": self.best_feature_model_results,\n",
    "            \"Best Params Model\": self.best_params_model_results,\n",
    "        }\n",
    "\n",
    "        summary_frames = [summarize_metric_results(results).assign(Model=name) for name, results in model_results.items()]\n",
    "\n",
    "        return pd.concat(summary_frames, ignore_index=True)\n",
    "\n",
    "    def get_result_analysis(self) -> None:\n",
    "    \n",
    "        analyze_model(\n",
    "            \"base_model\", \n",
    "            self.base_model, \n",
    "            self.base_model_results, \n",
    "            self.X_train.columns.tolist(), \n",
    "            self.X_train, \n",
    "            self.y_train, \n",
    "            self.y_test, \n",
    "            self.target, \n",
    "            self.scorer\n",
    "        )\n",
    "        analyze_model(\n",
    "            \"best_feature_model\", \n",
    "            self.best_feature_model, \n",
    "            self.best_feature_model_results, \n",
    "            self.best_features, \n",
    "            self.X_train, \n",
    "            self.y_train, \n",
    "            self.y_test, \n",
    "            self.target, \n",
    "            self.scorer\n",
    "        )\n",
    "        analyze_model(\n",
    "            \"best_params_model\", \n",
    "            self.best_params_model, \n",
    "            self.best_params_model_results,\n",
    "            self.best_features, \n",
    "            self.X_train, \n",
    "            self.y_train, \n",
    "            self.y_test, \n",
    "            self.target, \n",
    "            self.scorer\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b13e89-793a-4fc3-8c4c-cb0b96a895bc",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0981db97-1ea4-4211-ba8e-fe21ee0bd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_ucirepo(id=350) \n",
    "\n",
    "X = df.data.features\n",
    "df_aux = df.variables\n",
    "X.columns = df_aux.loc[~df_aux['name'].isin(['ID', 'Y']), 'description'].values\n",
    "\n",
    "y = df.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8253d86-b2da-45ac-8833-3a5cfd69a818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Y\n",
       "0    0.7788\n",
       "1    0.2212\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['Y'].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61f6bd46-6231-40f0-84bb-649752040093",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97ccd0-dbbb-4b12-a677-b7e6ca00961e",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cf10f-cac4-4b5b-9b07-c28fdfde4056",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbee4cc-4982-46e9-a255-cc32c1816230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfs = SequentialFeatureSelector(\n",
    "    estimator=LGBMClassifier(verbosity=-1), \n",
    "    n_features_to_select='auto',\n",
    "    direction='forward',\n",
    "    scoring='roc_auc', \n",
    "    cv=3, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155de38-e8d7-4748-bac3-f0a86a670080",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:, sfs.get_support()].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586b232-aa91-46be-a481-a9dc8d72810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['LIMIT_BAL', 'EDUCATION', 'PAY_0', 'PAY_2', 'PAY_4', 'PAY_5', 'BILL_AMT1', 'BILL_AMT3', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT6']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba3db8-c7cb-441e-824b-7f88c052f5ef",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1ce69-3639-4b09-8131-0702a2be161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_lgbm = AutoMLLGBMClassifierCV(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    best_features=selected_columns,\n",
    "    target='Y',\n",
    "    scoring='brier',\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dea218-5f0b-4de3-b9bd-6c87c0e828c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_lgbm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d814e75-eb06-4686-82d0-78bc65dcb167",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_lgbm.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bdb15a-5024-4e31-9ee3-a87ce3bae194",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_lgbm.get_result_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948d266-bd76-47dc-adf8-e81739556a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eadc67-ffcb-4db9-a592-77e4108a944a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f6b00-4679-40bc-959b-4673a31180d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
